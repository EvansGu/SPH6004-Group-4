{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"df_imputed_nodrop.csv\")\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimated mean and median ICU length of stay\n",
    "# Convert 'icu_intime' and 'icu_outtime' columns to datetime\n",
    "data['icu_intime'] = pd.to_datetime(data['icu_intime'])\n",
    "data['icu_outtime'] = pd.to_datetime(data['icu_outtime'])\n",
    "\n",
    "# Calculate ICU stay length in days for each patient\n",
    "data['icu_stay_length'] = (data['icu_outtime'] - data['icu_intime']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# Display statistics on ICU stay lengths\n",
    "icu_stay_statistics = data['icu_stay_length'].describe()\n",
    "icu_stay_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9fdd67",
   "metadata": {},
   "source": [
    "Death Patient Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff497a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with negative ICU stay lengths\n",
    "data_cleaned = data[data['icu_stay_length'] >= 0]\n",
    "\n",
    "# Filter out the patients who died in ICU\n",
    "data_icu_deaths = data_cleaned[data_cleaned['icu_death'] == 1]\n",
    "\n",
    "# Checking the cleaned and filtered data\n",
    "data_icu_deaths.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a time window with a length of 3\n",
    "\n",
    "# Ensure 'charttime' is in datetime format\n",
    "data_icu_deaths['charttime'] = pd.to_datetime(data_icu_deaths['charttime'])\n",
    "\n",
    "# Sort by patient ID and charttime to ensure data is in chronological order\n",
    "data_icu_deaths_sorted = data_icu_deaths.sort_values(by=['id', 'charttime'])\n",
    "\n",
    "# Define a function to create rolling windows of 3 days\n",
    "def create_rolling_windows_with_avg(df, window_size=3):\n",
    "    windows = []\n",
    "    avg_los_icu = []\n",
    "    for patient_id, group in df.groupby('id'):\n",
    "        for start_index in range(len(group) - window_size + 1):\n",
    "            window = group.iloc[start_index:start_index + window_size]\n",
    "            if (window['charttime'].iloc[-1] - window['charttime'].iloc[0]).days < window_size:\n",
    "                windows.append(window)\n",
    "                avg_los_icu.append(window['los_icu'].mean())  # 计算窗口内 los_icu 的平均值\n",
    "    return windows, avg_los_icu\n",
    "\n",
    "# Apply this function to create time windows and averages\n",
    "time_windows, avg_los_icu = create_rolling_windows_with_avg(data_icu_deaths_sorted)\n",
    "\n",
    "# Example of how many windows we have and a peek at the first window data\n",
    "len(time_windows), time_windows[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric columns for correlation computation\n",
    "numeric_data = data_icu_deaths.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# Find the top 30 features most correlated with los_icu, excluding 'id' and 'los_icu' itself\n",
    "top_features = correlation_matrix['los_icu'].drop(['id', 'los_icu']).abs().sort_values(ascending=False).head(31)\n",
    "\n",
    "# Display the top 30 features\n",
    "top_features = top_features.iloc[1:]\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d8d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = top_features.index.tolist()\n",
    "\n",
    "# Re-filtering the data to include only the selected top features, plus 'los_icu', 'charttime', 'id'\n",
    "filtered_data_windows = [window[selected_features + ['los_icu', 'charttime', 'id']] for window in time_windows]\n",
    "\n",
    "# Combine all windows into a single DataFrame\n",
    "combined_data = pd.concat(filtered_data_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b171d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(combined_data[selected_features])\n",
    "scaled_target = combined_data['los_icu'].values\n",
    "\n",
    "# Convert the list of averages to a numpy array for subsequent processing\n",
    "y = np.array(avg_los_icu)\n",
    "\n",
    "# Make sure the shape of the data and labels are consistent\n",
    "X = scaled_features.reshape(len(time_windows), len(time_windows[0]), len(selected_features))\n",
    "y = y.reshape(-1, 1)  # Make sure y is a two-dimensional array that adapts to the model output\n",
    "\n",
    "# Split the data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Shapes of the training and test datasets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust the model architecture to ensure that the output layer is suitable for average prediction\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Model training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "history = model.fit(X_train, y_train, epochs=200, validation_split=0.2, callbacks=[early_stopping], batch_size=32)\n",
    "\n",
    "# Model evaluation\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error, mean absolute error and coefficient of determination\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "\n",
    "print(f'Test MSE: {mse}')\n",
    "print(f'Test MAE: {mae}')\n",
    "print(\"R-squared (Coefficient of Determination):\", r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomizedSearchCV provides an alternative to GridSearchCV that instead of trying all parameter combinations, randomly selects from a specified parameter distribution. This can significantly reduce the number of configurations that must be evaluated.from scikeras.wrappers import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "def build_model(lstm_units=50, activation='relu', optimizer='adam'):\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, activation=activation, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "model = KerasRegressor(model=build_model, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "param_distributions = {\n",
    "    'model__lstm_units': [20, 50, 100],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'model__optimizer': ['adam', 'rmsprop'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [50, 100]\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42)\n",
    "\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (rand_search.best_score_, rand_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00808e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust the model architecture to ensure that the output layer is suitable for average prediction\n",
    "model = Sequential([\n",
    "    LSTM(100, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "history = model.fit(X_train, y_train, epochs=200, validation_split=0.2, callbacks=[early_stopping], batch_size=64)\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "\n",
    "print(f'Test MSE: {mse}')\n",
    "print(f'Test MAE: {mae}')\n",
    "print(\"R-squared (Coefficient of Determination):\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a40d2d",
   "metadata": {},
   "source": [
    "Non-death Patient Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c594f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with negative ICU stay lengths\n",
    "data_cleaned = data[data['icu_stay_length'] >= 0]\n",
    "\n",
    "# Filter out the patients who died in ICU\n",
    "data_icu_survived = data_cleaned[data_cleaned['icu_death'] == 0]\n",
    "\n",
    "# Checking the cleaned and filtered data\n",
    "data_icu_survived.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a time window with a length of 3\n",
    "\n",
    "# Ensure 'charttime' is in datetime format\n",
    "data_icu_survived['charttime'] = pd.to_datetime(data_icu_survived['charttime'])\n",
    "\n",
    "# Sort by patient ID and charttime to ensure data is in chronological order\n",
    "data_icu_survived_sorted = data_icu_survived.sort_values(by=['id', 'charttime'])\n",
    "\n",
    "# Define a function to create rolling windows of 3 days\n",
    "def create_rolling_windows_with_avg(df, window_size=3):\n",
    "    windows = []\n",
    "    avg_los_icu = []\n",
    "    for patient_id, group in df.groupby('id'):\n",
    "        for start_index in range(len(group) - window_size + 1):\n",
    "            window = group.iloc[start_index:start_index + window_size]\n",
    "            if (window['charttime'].iloc[-1] - window['charttime'].iloc[0]).days < window_size:\n",
    "                windows.append(window)\n",
    "                avg_los_icu.append(window['los_icu'].mean())  # 计算窗口内 los_icu 的平均值\n",
    "    return windows, avg_los_icu\n",
    "\n",
    "# Apply this function to create time windows and averages\n",
    "time_windows, avg_los_icu = create_rolling_windows_with_avg(data_icu_survived_sorted)\n",
    "\n",
    "# Example of how many windows we have and a peek at the first window data\n",
    "len(time_windows), time_windows[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b113218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric columns for correlation computation\n",
    "numeric_data = data_icu_survived.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# Find the top 30 features most correlated with los_icu, excluding 'id' and 'los_icu' itself\n",
    "top_features = correlation_matrix['los_icu'].drop(['id', 'los_icu']).abs().sort_values(ascending=False).head(31)\n",
    "\n",
    "# Display the top 30 features\n",
    "top_features = top_features.iloc[1:]\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = top_features.index.tolist()\n",
    "\n",
    "# Re-filtering the data to include only the selected top features, plus 'los_icu', 'charttime', 'id'\n",
    "filtered_data_windows = [window[selected_features + ['los_icu', 'charttime', 'id']] for window in time_windows]\n",
    "\n",
    "# Combine all windows into a single DataFrame\n",
    "combined_data = pd.concat(filtered_data_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72634a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(combined_data[selected_features])\n",
    "scaled_target = combined_data['los_icu'].values\n",
    "\n",
    "# Convert the list of averages to a numpy array for subsequent processing\n",
    "y = np.array(avg_los_icu)\n",
    "\n",
    "X = scaled_features.reshape(len(time_windows), len(time_windows[0]), len(selected_features))\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Split the data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Shapes of the training and test datasets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1986e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "def build_model(lstm_units=50, activation='relu', optimizer='adam'):\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, activation=activation, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "model = KerasRegressor(model=build_model, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "param_distributions = {\n",
    "    'model__lstm_units': [20, 50, 100],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'model__optimizer': ['adam', 'rmsprop'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [50, 100]\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "\n",
    "rand_result = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (rand_result.best_score_, rand_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0587dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust the model architecture to ensure that the output layer is suitable for average prediction\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(100, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Model training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "history = model.fit(X_train, y_train, epochs=200, validation_split=0.2, callbacks=[early_stopping], batch_size=32)\n",
    "\n",
    "# Model evaluation\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error, mean absolute error and coefficient of determination\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "\n",
    "print(f'Test MSE: {mse}')\n",
    "print(f'Test MAE: {mae}')\n",
    "print(\"R-squared (Coefficient of Determination):\", r_squared)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "原始单元格格式",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
